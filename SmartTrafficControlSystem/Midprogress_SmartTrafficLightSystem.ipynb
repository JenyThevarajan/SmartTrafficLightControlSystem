{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8191ba",
   "metadata": {},
   "source": [
    "# CANNY EDGE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98353166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"p1.jpg\")\n",
    "\n",
    "edges = cv2.Canny(img,300,300)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.imshow(\"edges\",edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7fac1",
   "metadata": {},
   "source": [
    "# COUNTING VEHICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5793567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vehicles: 2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('cars.png')  # Replace 'your_image.jpg' with your image file\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur to reduce noise\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(blurred, 50, 150)  # Adjust the threshold values as needed\n",
    "\n",
    "# Find contours in the edge image\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Initialize a variable to count vehicles\n",
    "vehicle_count = 0\n",
    "\n",
    "# Iterate through the contours and filter potential vehicle contours\n",
    "for contour in contours:\n",
    "    # You can apply various criteria here to filter vehicle contours based on size, aspect ratio, etc.\n",
    "    if cv2.contourArea(contour) > 1000:  # Adjust the minimum contour area as needed\n",
    "        vehicle_count += 1\n",
    "\n",
    "# Display the count of vehicles\n",
    "print(f\"Number of vehicles: {vehicle_count}\")\n",
    "\n",
    "# Draw bounding boxes around the detected vehicles (optional)\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Display the original image with bounding boxes (optional)\n",
    "cv2.imshow('Vehicle Detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37beff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vehicles detected: 22\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('p1.jpg')\n",
    "#image = cv2.imread('enhanced_image.jpg')\n",
    "\n",
    "# Resize the image (optional)\n",
    "resized_image = cv2.resize(image, (800, 600))\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur to reduce noise\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "edges = cv2.Canny(blurred, 50, 150)\n",
    "\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load the traffic junction image\n",
    "input_image = cv2.imread('p1.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 1: Apply adaptive thresholding\n",
    "adaptive_threshold = cv2.adaptiveThreshold(\n",
    "    gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, blockSize=11, C=2)\n",
    "\n",
    "# Step 2: Find contours in the thresholded image\n",
    "contours, _ = cv2.findContours(adaptive_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Step 3: Filter contours based on area\n",
    "min_contour_area = 175  # Adjust this threshold as needed\n",
    "vehicle_count = 0\n",
    "\n",
    "for contour in contours:\n",
    "    if cv2.contourArea(contour) > min_contour_area:\n",
    "        # Increment the vehicle count for each significant contour\n",
    "        vehicle_count += 1\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "# Step 4: Display and print the vehicle count\n",
    "cv2.drawContours(input_image, contours, -1, (0, 255, 0), 2)\n",
    "cv2.putText(input_image, f'Vehicle Count: {vehicle_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "cv2.imshow('Vehicle Counting', input_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'Total vehicles detected: {vehicle_count}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd08850",
   "metadata": {},
   "source": [
    "min_contour_area = 175  # Adjust this threshold as needed\n",
    "vehicle_count = 0\n",
    "\n",
    "for contour in contours:\n",
    "    if cv2.contourArea(contour) > min_contour_area:\n",
    "        # Increment the vehicle count for each significant contour\n",
    "        vehicle_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723f5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('day1.png')\n",
    "\n",
    "# Preprocess the image (convert to grayscale and apply Gaussian blur)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(blurred, threshold1=20, threshold2=80)\n",
    "\n",
    "# Find contours in the edges\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter and process the contours (e.g., based on size or aspect ratio)\n",
    "\n",
    "# Draw the detected contours on the original image\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Vehicle Detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c19a2dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Perform car detection\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m cars \u001b[38;5;241m=\u001b[39m \u001b[43mcar_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Draw rectangles around detected cars and count them\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m cars:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained car detection model\n",
    "car_cascade = cv2.CascadeClassifier('haarcascade_car.xml')\n",
    "\n",
    "# Load the traffic junction image\n",
    "image = cv2.imread('nightcar.png')\n",
    "\n",
    "# Convert the image to grayscale for detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform car detection\n",
    "cars = car_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Draw rectangles around detected cars and count them\n",
    "for (x, y, w, h) in cars:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# Display the image with detected cars and count\n",
    "vehicle_count = len(cars)\n",
    "cv2.putText(image, f'Vehicle Count: {vehicle_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('Vehicle Detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the vehicle count\n",
    "print(f'Total vehicles detected: {vehicle_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b96d76",
   "metadata": {},
   "source": [
    "# BRIGHTNESS INCREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177268d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('c1.png')\n",
    "\n",
    "# Define a brightness factor (adjust this value as needed)\n",
    "brightness_factor = 3.0  # Increase this value to make the image brighter\n",
    "\n",
    "# Apply brightness enhancement by multiplying pixel values\n",
    "enhanced_image = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)\n",
    "\n",
    "# Display the original and enhanced images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the enhanced image\n",
    "cv2.imwrite('enhanced_image.jpg', enhanced_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf80d3",
   "metadata": {},
   "source": [
    "# Consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600e56ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def enhance_low_light_image(input_image):\n",
    "    # Convert the input image to LAB color space\n",
    "    lab_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # Separate the LAB channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "    \n",
    "    # Enhance the luminance (L) channel (illumination component)\n",
    "    l_channel_enhanced = cv2.equalizeHist(l_channel)\n",
    "    \n",
    "    # Merge the enhanced L channel with the original A and B channels\n",
    "    enhanced_lab_image = cv2.merge((l_channel_enhanced, a_channel, b_channel))\n",
    "    \n",
    "    # Convert the enhanced LAB image back to BGR color space\n",
    "    enhanced_image = cv2.cvtColor(enhanced_lab_image, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return enhanced_image\n",
    "\n",
    "# Load the low-light image\n",
    "input_image1 = cv2.imread('c1.png')\n",
    "input_image2 = cv2.imread('c2.png')\n",
    "input_image3 = cv2.imread('nightcar.png')\n",
    "\n",
    "# Apply the Dual Channel Prior-based enhancement\n",
    "enhanced_image1 = enhance_low_light_image(input_image1)\n",
    "enhanced_image2 = enhance_low_light_image(input_image2)\n",
    "enhanced_image3 = enhance_low_light_image(input_image3)\n",
    "\n",
    "# Display and save the enhanced image\n",
    "cv2.imshow('Enhanced Image1', enhanced_image1)\n",
    "cv2.imshow('Enhanced Image2', enhanced_image2)\n",
    "cv2.imshow('Enhanced Image3', enhanced_image3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('enhanced_image1.jpg', enhanced_image1)\n",
    "cv2.imwrite('enhanced_image2.jpg', enhanced_image2)\n",
    "cv2.imwrite('enhanced_image3.jpg', enhanced_image3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f012a",
   "metadata": {},
   "source": [
    "# # Normal brightning image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab473f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def dual_channel_prior_enhancement(image):\n",
    "    # Convert the input image to YUV color space\n",
    "    yuv_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Split the YUV image into separate channels\n",
    "    channels = list(cv2.split(yuv_image))  # Convert to a list to allow modification\n",
    "\n",
    "    # Apply histogram equalization to the Y channel for enhancing luminance\n",
    "    channels[0] = cv2.equalizeHist(channels[0])\n",
    "\n",
    "    # Merge the enhanced channels back to the YUV image\n",
    "    enhanced_yuv_image = cv2.merge(channels)\n",
    "\n",
    "    # Convert the enhanced YUV image back to BGR color space\n",
    "    enhanced_image = cv2.cvtColor(enhanced_yuv_image, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "# Load the low-light image\n",
    "input_image = cv2.imread('n2.png')\n",
    "\n",
    "# Apply the Dual Channel Prior enhancement\n",
    "enhanced_image = dual_channel_prior_enhancement(input_image)\n",
    "\n",
    "# Display the enhanced image\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the enhanced image\n",
    "cv2.imwrite('enhanced_nightcar31.png', enhanced_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae061c",
   "metadata": {},
   "source": [
    "# DUAL CHANNEL PRIOR METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01030fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the low-light image\n",
    "input_image = cv2.imread('nightcar.png')\n",
    "\n",
    "# Step 1: Find maximum and minimum pixel values (local patch)\n",
    "patch_size = 1  # Size of the local patch for finding max and min values\n",
    "max_values = cv2.erode(input_image, np.ones((patch_size, patch_size), np.uint8))\n",
    "min_values = cv2.dilate(input_image, np.ones((patch_size, patch_size), np.uint8))\n",
    "\n",
    "# Step 2: Find bright channel and dark channel\n",
    "bright_channel = max_values\n",
    "dark_channel = min_values\n",
    "\n",
    "# Step 3: Transmission estimation (initial transmission map)\n",
    "Airlight_percentage = 0.001  # Percentage of the brightest pixels to use for Airlight estimation\n",
    "sorted_bright = np.sort(bright_channel.ravel())\n",
    "Airlight = sorted_bright[-int(Airlight_percentage * len(sorted_bright))]\n",
    "transmission = 1 - (dark_channel / Airlight)\n",
    "\n",
    "# Step 4: Darkness estimation (darkness weights)\n",
    "darkness_weights = 1 - transmission\n",
    "\n",
    "# Step 5: Corrected transmission map (convolution)\n",
    "window_size = 21  # Size of the window for convolution\n",
    "corrected_transmission = cv2.blur(transmission, (window_size, window_size))\n",
    "\n",
    "# Step 6: Estimate atmospheric light\n",
    "num_pixels = int(input_image.size * Airlight_percentage)\n",
    "atmospheric_light = np.max(sorted_bright[-num_pixels:])\n",
    "\n",
    "# Step 7: Enhance the image\n",
    "\n",
    "# Define brightness factors for the brighter and darker channels\n",
    "brighter_factor = 3.0  # Increase this value to make the image brighter\n",
    "darker_factor = 0.5   # Decrease this value to make the image darker\n",
    "\n",
    "# Apply brightness enhancements for the brighter and darker channels\n",
    "brighter_image = cv2.convertScaleAbs(input_image, alpha=brighter_factor, beta=0)\n",
    "darker_image = cv2.convertScaleAbs(input_image, alpha=darker_factor, beta=0)\n",
    "\n",
    "enhanced_image = ((input_image - atmospheric_light) / np.maximum(corrected_transmission, 0.1)) + atmospheric_light\n",
    "enhanced_image = np.clip(enhanced_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Display the original, brighter, and darker images\n",
    "cv2.imshow('Original Image', input_image)\n",
    "cv2.imshow('Brighter Image', brighter_image)\n",
    "#cv2.imshow('Darker Image', darker_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# Display and save the enhanced image\n",
    "#cv2.imshow('Enhanced Image', enhanced_image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#cv2.imwrite('enhanced_image.jpg', enhanced_image)\n",
    "\n",
    "\n",
    "# Load the night-time traffic vehicle image\n",
    "#image = cv2.imread('nightcar.png')\n",
    "\n",
    "\n",
    "# Save the enhanced images\n",
    "#cv2.imwrite('brighter_image.jpg', brighter_image)\n",
    "#cv2.imwrite('darker_image.jpg', darker_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577802f",
   "metadata": {},
   "source": [
    "# DUAL CHANNEL PRIOR ENHANCEMENT METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3723b510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def dual_channel_prior_enhancement(image):\n",
    "    # Convert the input image to YUV color space\n",
    "    yuv_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Split the YUV image into separate channels\n",
    "    channels = list(cv2.split(yuv_image))  # Convert to a list to allow modification\n",
    "\n",
    "    # Apply histogram equalization to the Y channel for enhancing luminance\n",
    "    channels[0] = cv2.equalizeHist(channels[0])\n",
    "\n",
    "    # Merge the enhanced channels back to the YUV image\n",
    "    enhanced_yuv_image = cv2.merge(channels)\n",
    "\n",
    "    # Convert the enhanced YUV image back to BGR color space\n",
    "    enhanced_image = cv2.cvtColor(enhanced_yuv_image, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "# Load the low-light image\n",
    "input_image = cv2.imread('nightcar.png')\n",
    "\n",
    "# Apply the Dual Channel Prior enhancement\n",
    "enhanced_image = dual_channel_prior_enhancement(input_image)\n",
    "\n",
    "# Display the enhanced image\n",
    "cv2.imshow('Enhanced Image', enhanced_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the enhanced image\n",
    "cv2.imwrite('enhanced_nightcar31.png', enhanced_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75338f4",
   "metadata": {},
   "source": [
    "# SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d36a5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 224 frames into sampled_images\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"C:/Users/94774/Desktop/xyz.mp4\"  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create a directory to store the sampled images\n",
    "output_directory = 'sampled_images'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Set the frame sampling rate (e.g., 1 frame per second)\n",
    "frame_rate = 5  # Change as needed\n",
    "\n",
    "# Initialize variables\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Save the frame as an image\n",
    "    image_filename = os.path.join(output_directory, f'frame_{frame_count:04d}.jpg')\n",
    "    cv2.imwrite(image_filename, frame)\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames to sample at the desired frame rate\n",
    "    skip_frames = int(cap.get(cv2.CAP_PROP_FPS) // frame_rate) - 1\n",
    "    for _ in range(skip_frames):\n",
    "        cap.read()\n",
    "\n",
    "cap.release()\n",
    "\n",
    "print(f'Sampled {frame_count} frames into {output_directory}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f197f1b",
   "metadata": {},
   "source": [
    "# SEGMENTING IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2cb0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open the image\n",
    "image = cv2.imread('nightcar.png')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "height, width, _ = image.shape  # Use .shape for NumPy arrays\n",
    "\n",
    "# Calculate the dimensions of each part\n",
    "part_width = width // 3\n",
    "part_height = height\n",
    "\n",
    "# Crop the image into three equal parts\n",
    "part1 = Image.fromarray(image[:, :part_width, :])  # Convert the NumPy array to a PIL Image\n",
    "part2 = Image.fromarray(image[:, part_width:2 * part_width, :])\n",
    "part3 = Image.fromarray(image[:, 2 * part_width:, :])\n",
    "\n",
    "# Save the three parts as separate images\n",
    "part1.save(\"part1.jpg\")\n",
    "part2.save(\"part2.jpg\")\n",
    "part3.save(\"part3.jpg\")\n",
    "\n",
    "# Convert PIL Images back to NumPy arrays\n",
    "part1_np = np.array(part1)\n",
    "part2_np = np.array(part2)\n",
    "part3_np = np.array(part3)\n",
    "\n",
    "# Display the images using cv2.imshow\n",
    "cv2.imshow('part1.jpg', part1_np)\n",
    "cv2.imshow('part2.jpg', part2_np)\n",
    "cv2.imshow('part3.jpg', part3_np)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90940341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
